{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27406eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import webdataset as wds\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import re\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pixyz.distributions import Normal, Bernoulli, Laplace, ProductOfNormal\n",
    "from pixyz.models import Model\n",
    "from pixyz.losses import KullbackLeibler\n",
    "from pixyz.losses import LogProb\n",
    "from pixyz.losses import Expectation as E\n",
    "from pixyz.losses import Parameter\n",
    "from pixyz.utils import print_latex\n",
    "from pixyz.utils import epsilon\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9098e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/acg17270jl/projects/fmri-reconstruction-with-dmvae/data/mindeye2_nsd/\"\n",
    "subj_list = [1, 2, 5, 7] # [1, 2, 3, 4, 5, 6, 7, 8] \n",
    "all_subj_num_ssessions_list = [40, 40, 32, 30, 40, 32, 40, 30]\n",
    "all_subj_lambdas = [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "batch_size = 128\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83e390",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5py' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfmri_reconstruction_with_dmvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmindeye2_nsd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_all_subj_data, load_all_subj_voxels\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#all_subj_shared1000_data = load_all_subj_data(data_path, subj_list, data_range=\"shared1000\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# all_subj_all_data = load_all_subj_data(data_path, subj_list, data_range=\"all\", subj_num_ssessions_list=all_subj_num_ssessions_list)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m all_subj_voxels, all_subj_num_voxels \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_subj_voxels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubj_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/fmri-reconstruction-with-dmvae/src/fmri_reconstruction_with_dmvae/mindeye2_nsd/datasets/load.py:44\u001b[0m, in \u001b[0;36mload_all_subj_voxels\u001b[0;34m(data_path, subj_list)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subj \u001b[38;5;129;01min\u001b[39;00m subj_list:\n\u001b[1;32m     43\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(subj)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 44\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/betas_all_subj\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fp32_renorm.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m     betas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m][:])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m     all_subj_voxels[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubj\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m betas\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h5py' is not defined"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "\n",
    "def load_all_subj_data(data_path, subj_list, data_range, subj_num_ssessions_list=None):\n",
    "      all_subj_data = {}\n",
    "\n",
    "      for s in subj_list:\n",
    "          if data_range == \"no-shared1000\":\n",
    "              data_url = f\"{data_path}/wds/subj0{s}/train/\" \"{0..\" + f\"{subj_num_ssessions_list[s-1]-1}\" + \"}.tar\"\n",
    "          elif data_range == \"shared1000\":\n",
    "              data_url = f\"{data_path}/wds/subj0{s}/new_test/\" + \"0.tar\"\n",
    "          elif data_range == \"all\":\n",
    "              data_url = (\n",
    "                  f\"{data_path}/wds/subj0{s}/train/\" \"{0..\" + f\"{subj_num_ssessions_list[s-1]-1}\" + \"}.tar::\"\n",
    "                  f\"{data_path}/wds/subj0{s}/new_test/0.tar\"\n",
    "              )\n",
    "          else:\n",
    "              raise ValueError(f\"Unsupported data_range: {data_range}\")\n",
    "\n",
    "          subj_iter_data = wds.WebDataset(data_url, resampled=False, shardshuffle=False, nodesplitter=my_split_by_node) \\\n",
    "                              .decode(\"torch\") \\\n",
    "                              .rename(behav=\"behav.npy\",\n",
    "                                      past_behav=\"past_behav.npy\",\n",
    "                                      future_behav=\"future_behav.npy\",\n",
    "                                      olds_behav=\"olds_behav.npy\") \\\n",
    "                              .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "\n",
    "          # global_trial (behav[0, 5]) で昇順ソート\n",
    "          subj_data = list(subj_iter_data)\n",
    "          subj_data = sorted(subj_data, key=lambda sample: sample[0][0, 5].item())\n",
    "          all_subj_data[f\"subj0{s}\"] = subj_data\n",
    "\n",
    "      print(\"Loaded all subj data\\n\")\n",
    "      \n",
    "      return all_subj_data\n",
    "\n",
    "def load_all_subj_voxels(data_path, subj_list):\n",
    "    all_subj_voxels = {}\n",
    "    all_subj_num_voxels = {}\n",
    "\n",
    "    for subj in subj_list:\n",
    "        s = f\"{int(subj):02d}\"\n",
    "        f = h5py.File(f\"{data_path}/betas_all_subj{s}_fp32_renorm.hdf5\", \"r\")\n",
    "        betas = torch.Tensor(f[\"betas\"][:]).to(\"cpu\")\n",
    "        all_subj_voxels[f\"subj{s}\"] = betas\n",
    "        all_subj_num_voxels[f\"subj{s}\"] = betas[0].shape[-1]\n",
    "\n",
    "    print(\"Loaded all subj voxels\\n\")\n",
    "    \n",
    "    return all_subj_voxels, all_subj_num_voxels\n",
    "\n",
    "all_subj_shared1000_data = load_all_subj_data(data_path, subj_list, data_range=\"shared1000\")\n",
    "# all_subj_all_data = load_all_subj_data(data_path, subj_list, data_range=\"all\", subj_num_ssessions_list=all_subj_num_ssessions_list)\n",
    "\n",
    "all_subj_voxels, all_subj_num_voxels = load_all_subj_voxels(data_path, subj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fmri_reconstruction_with_dmvae.mindeye2_nsd.datasets.align import align_subject_trials\n",
    "\n",
    "aligned_all_subj_shared1000_data = align_subject_trials(all_subj_shared1000_data, anchor_subject=\"subj01\")\n",
    "# aligned_all_subj_data = align_subject_trials(all_subj_all_data, anchor_subject=\"subj01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba74a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_aligned_data(\n",
    "      aligned_data: List[Dict[str, object]],\n",
    "      subj_list: Iterable[int],\n",
    "      train_occurrence_max: int = 2,\n",
    "  ) -> Tuple[\n",
    "      List[Tuple[int, Dict[str, Optional[int]]]],\n",
    "      List[Tuple[int, Dict[str, Optional[int]]]],\n",
    "  ]:\n",
    "      \"\"\"\n",
    "      aligned_data: align_subject_trials の結果（各要素に cocoidx / occurrence / subjXX フィールドがある）\n",
    "      subj_list: 例 [1, 2, 3] のような被験者 ID リスト\n",
    "      戻り値: (train_data, test_data)\n",
    "        - 各 data[i] は (cocoidx, {\"subj01\": global_trial or None, ...})\n",
    "      \"\"\"\n",
    "      train_data: List[Tuple[int, Dict[str, Optional[int]]]] = []\n",
    "      test_data: List[Tuple[int, Dict[str, Optional[int]]]] = []\n",
    "\n",
    "      for data in aligned_data:\n",
    "          cocoidx = int(data[\"cocoidx\"])\n",
    "          global_trials: Dict[str, Optional[int]] = {}\n",
    "\n",
    "          for subj in subj_list:\n",
    "              s = f\"subj{int(subj):02d}\"\n",
    "              subject_info = data.get(s)\n",
    "              global_trials[s] = (\n",
    "                  subject_info[\"global_trial\"] if subject_info is not None else None\n",
    "              )\n",
    "\n",
    "          sample = (cocoidx, global_trials)\n",
    "          occurrence = int(data[\"occurrence\"])\n",
    "\n",
    "          if occurrence <= train_occurrence_max:\n",
    "              train_data.append(sample)\n",
    "          else:\n",
    "              test_data.append(sample)\n",
    "\n",
    "      return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "502cd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_aligned_data(aligned_all_subj_shared1000_data, subj_list, train_occurrence_max=2)\n",
    "# train_data, test_data = split_aligned_data(aligned_all_subj_data, subj_list, train_occurrence_max=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9af134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StimulusTrialMappingDataset(Dataset):\n",
    "      \"\"\"\n",
    "      (cocoidx, {\"subjXX\": global_trial or None, ...}) のリストをそのまま扱う Dataset。\n",
    "      \"\"\"\n",
    "      def __init__(self, data: List[Tuple[int, Dict[str, Optional[int]]]]):\n",
    "          self.data = data\n",
    "\n",
    "      def __len__(self) -> int:\n",
    "          return len(self.data)\n",
    "\n",
    "      def __getitem__(self, idx: int) -> Tuple[int, Dict[str, Optional[int]]]:\n",
    "          return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "671f72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StimulusTrialMappingDataset(train_data)\n",
    "test_dataset = StimulusTrialMappingDataset(test_data)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee65a7",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf6afb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(x_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.fc(x))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, x_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eae19a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference(Normal):\n",
    "    def __init__(self, enc, var, cond_var, z_dim, hidden_dim=1024):\n",
    "        super(Inference, self).__init__(var=var, cond_var=cond_var, name=\"q\")\n",
    "\n",
    "        self.enc = enc\n",
    "        self.mu = nn.Linear(hidden_dim, z_dim)\n",
    "        self.logvar = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, **x):\n",
    "        x = torch.cat([x[_cond_var] for _cond_var in self.cond_var], dim=1)\n",
    "        h = self.enc(x)\n",
    "\n",
    "        return {\"loc\": self.mu(h), \"scale\": F.softplus(self.logvar(h)) + epsilon()}\n",
    "\n",
    "class Generator(Laplace):\n",
    "    def __init__(self, dec, var, cond_var, zp_dim, zs_dim, hidden_dim=1024):\n",
    "        super(Generator, self).__init__(var=var, cond_var=cond_var, name=\"p\")\n",
    "\n",
    "        self.fc = nn.Linear(zp_dim + zs_dim, hidden_dim)\n",
    "        self.dec = dec\n",
    "\n",
    "    def forward(self, **z):\n",
    "        z = torch.cat([z[_cond_var] for _cond_var in self.cond_var], dim=1)\n",
    "        h = F.relu(self.fc(z))\n",
    "\n",
    "        return {\"loc\": self.dec(h), \"scale\": torch.tensor(1.0).to(z.device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "058b38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 1024\n",
    "\n",
    "enc_dict = {}\n",
    "dec_dict = {}\n",
    "\n",
    "for subj in subj_list:\n",
    "    s = f\"{int(subj):02d}\"\n",
    "    \n",
    "    enc_dict[f\"subj{s}_zp\"] = Encoder(x_dim=all_subj_num_voxels[f\"subj{s}\"], hidden_dim=hidden_dim)\n",
    "    enc_dict[f\"subj{s}_zs\"] = Encoder(x_dim=all_subj_num_voxels[f\"subj{s}\"], hidden_dim=hidden_dim)\n",
    "    dec_dict[f\"subj{s}\"] = Decoder(x_dim=all_subj_num_voxels[f\"subj{s}\"], hidden_dim=hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b6aa4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "zp_dim = 256\n",
    "zs_dim = 768\n",
    "\n",
    "dist_dict = {}\n",
    "for subj in subj_list:\n",
    "    s = f\"{int(subj):02d}\"\n",
    "    \n",
    "    # q_φ(zp_subj | x_subj): q_φ1(zp1 | x1), q_φ2(zp2 | x2), ...\n",
    "    dist_dict[f\"q_zp{s}__x{s}\"] = Inference(enc=enc_dict[f\"subj{s}_zp\"], var=[f\"zp{s}\"], cond_var=[f\"x{s}\"], z_dim=zp_dim, hidden_dim=hidden_dim).to(device)\n",
    "    \n",
    "    # q_φ(zs_subj | x_subj): q_φ1(zs1 | x1), q_φ2(zs2 | x2), ...\n",
    "    dist_dict[f\"q_zs__x{s}\"] = Inference(enc=enc_dict[f\"subj{s}_zs\"], var=[\"zs\"], cond_var=[f\"x{s}\"], z_dim=zs_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "    # prior(zp_subj): prior(zp1), prior(zp2), ...\n",
    "    dist_dict[f\"prior_zp{s}\"] = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.), var=[f\"zp{s}\"], features_shape=[zp_dim], name=\"p_{prior}\").to(device)\n",
    "\n",
    "    # p_θ(x_subj | zp_subj, zs): p_θ1(x1 | zp1, zs), p_θ2(x2 | zp2, zs), ...\n",
    "    dist_dict[f\"p_x{s}__zp{s}_zs\"] = Generator(dec=dec_dict[f\"subj{s}\"], var=[f\"x{s}\"], cond_var=[f\"zp{s}\", \"zs\"], zp_dim=zp_dim, zs_dim=zs_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "\n",
    "# q_φ(zs | x)\n",
    "dist_dict[\"q_zs__x\"] = ProductOfNormal([dist_dict[f\"q_zs__x{int(subj):02d}\"] for subj in subj_list], name=\"q\").to(device)\n",
    "\n",
    "# prior(zs)\n",
    "dist_dict[\"prior_zs\"] = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.), var=[\"zs\"], features_shape=[zs_dim], name=\"p_{prior}\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa478d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "\n",
    "for subj_target in subj_list:\n",
    "    loss_subj_target = 0\n",
    "\n",
    "    s_t = f\"{int(subj_target):02d}\"\n",
    "\n",
    "    lambda_subj = Parameter(f\"lambda_{s_t}\")\n",
    "\n",
    "    joint_recon_loss = - lambda_subj * E(dist_dict[f\"q_zp{s_t}__x{s_t}\"], E(dist_dict[\"q_zs__x\"], LogProb(dist_dict[f\"p_x{s_t}__zp{s_t}_zs\"])))\n",
    "    joint_recon_kl = KullbackLeibler(dist_dict[f\"q_zp{s_t}__x{s_t}\"], dist_dict[f\"prior_zp{s_t}\"]) + KullbackLeibler(dist_dict[f\"q_zs__x\"], dist_dict[f\"prior_zs\"])\n",
    "    loss_subj_target += joint_recon_loss + joint_recon_kl\n",
    "\n",
    "    for subj_source in subj_list:\n",
    "        s_s = f\"{int(subj_source):02d}\"\n",
    "\n",
    "        if s_t == s_s:\n",
    "            self_recon_loss = - lambda_subj * E(dist_dict[f\"q_zp{s_t}__x{s_t}\"], E(dist_dict[f\"q_zs__x{s_s}\"], LogProb(dist_dict[f\"p_x{s_t}__zp{s_t}_zs\"])))\n",
    "            self_recon_kl = KullbackLeibler(dist_dict[f\"q_zp{s_t}__x{s_t}\"], dist_dict[f\"prior_zp{s_t}\"]) + KullbackLeibler(dist_dict[f\"q_zs__x{s_s}\"], dist_dict[\"prior_zs\"])\n",
    "            loss_subj_target += self_recon_loss + self_recon_kl\n",
    "        else:\n",
    "            cross_recon_loss = - lambda_subj * E(dist_dict[f\"q_zp{s_t}__x{s_t}\"], E(dist_dict[f\"q_zs__x{s_s}\"], LogProb(dist_dict[f\"p_x{s_t}__zp{s_t}_zs\"])))\n",
    "            cross_recon_kl = KullbackLeibler(dist_dict[f\"q_zp{s_t}__x{s_t}\"], dist_dict[f\"prior_zp{s_t}\"]) + KullbackLeibler(dist_dict[f\"q_zs__x{s_s}\"], dist_dict[\"prior_zs\"])\n",
    "            loss_subj_target += cross_recon_loss + cross_recon_kl\n",
    "            \n",
    "    loss += loss_subj_target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "009dc071",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(loss=loss, distributions=list(dist_dict.values()), optimizer=optim.Adam, optimizer_params={\"lr\": 1e-3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3233fb",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "acb3f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recon_dict_batch(input_dict):\n",
    "    z_dict = {}\n",
    "    recon_dict_batch = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for subj in subj_list:\n",
    "            s = f\"{int(subj):02d}\"\n",
    "            z_dict[f\"zp{s}\"] = dist_dict[f\"q_zp{s}__x{s}\"].sample(input_dict, return_all=False) \n",
    "            z_dict[f\"zs__x{s}\"] = dist_dict[f\"q_zs__x{s}\"].sample(input_dict, return_all=False)\n",
    "        \n",
    "        z_dict[\"zs__x\"] = dist_dict[\"q_zs__x\"].sample(input_dict, return_all=False)\n",
    "\n",
    "        for subj_target in subj_list:\n",
    "            s_t = f\"{int(subj_target):02d}\"\n",
    "\n",
    "            recon_dict_batch[f\"joint_recon_x{s_t}\"] = dist_dict[f\"p_x{s_t}__zp{s_t}_zs\"].sample_mean(z_dict[f\"zp{s_t}\"] | z_dict[f\"zs__x\"]).cpu()\n",
    "            \n",
    "            for subj_resource in subj_list:\n",
    "                s_s = f\"{int(subj_resource):02d}\"\n",
    "\n",
    "                if s_t == s_s:\n",
    "                    recon_dict_batch[f\"self_recon_x{s_t}\"] = dist_dict[f\"p_x{s_t}__zp{s_t}_zs\"].sample_mean(z_dict[f\"zp{s_t}\"] | z_dict[f\"zs__x{s_s}\"]).cpu()\n",
    "                else:\n",
    "                    recon_dict_batch[f\"cross_recon_x{s_t}__x{s_s}\"] = dist_dict[f\"p_x{s_t}__zp{s_t}_zs\"].sample_mean(z_dict[f\"zp{s_t}\"] | z_dict[f\"zs__x{s_s}\"]).cpu()\n",
    "\n",
    "    return recon_dict_batch\n",
    "\n",
    "def calc_cosine_dict_batch(input_dict, recon_dict_batch):\n",
    "    cosine_dict_batch = {}\n",
    "\n",
    "    for key in recon_dict_batch:\n",
    "        s_t = re.search(r'recon_x(\\d{2})', key).group(1)\n",
    "        cosine = F.cosine_similarity(input_dict[f\"x{s_t}\"].cpu(), recon_dict_batch[key])\n",
    "        cosine_dict_batch[key] = cosine\n",
    "\n",
    "    return cosine_dict_batch\n",
    "\n",
    "def calc_pearson_dict_batch(input_dict, recon_dict_batch):\n",
    "    pearson_dict_batch = {}\n",
    "\n",
    "    for key in recon_dict_batch:\n",
    "        s_t = re.search(r'recon_x(\\d{2})', key).group(1)\n",
    "        input_centered = input_dict[f\"x{s_t}\"] - input_dict[f\"x{s_t}\"].mean(dim=1, keepdim=True)\n",
    "        recon_centered = recon_dict_batch[key] - recon_dict_batch[key].mean(dim=1, keepdim=True)\n",
    "        pearson = F.cosine_similarity(input_centered.cpu(), recon_centered, dim=1)\n",
    "        pearson_dict_batch[key] = pearson\n",
    "    \n",
    "    return pearson_dict_batch\n",
    "\n",
    "def update_metrics(metrics_dict, input_dict, recon_dict_batch, n_batches):\n",
    "      cosine_dict = calc_cosine_dict_batch(input_dict, recon_dict_batch)\n",
    "      pearson_dict = calc_pearson_dict_batch(input_dict, recon_dict_batch)\n",
    "      \n",
    "      for key in recon_dict_batch:\n",
    "          entry = metrics_dict.setdefault(key, {\"cosine_mean\": 0.0, \"pearson_mean\": 0.0})\n",
    "          entry[\"cosine_mean\"] += cosine_dict[key].mean() / n_batches\n",
    "          entry[\"pearson_mean\"] += pearson_dict[key].mean() / n_batches\n",
    "\n",
    "      return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a12069f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    n_batches = len(train_dl)\n",
    "    train_metrics_dict = {}\n",
    "\n",
    "    for data in tqdm(train_dl):\n",
    "        input_dict = {}\n",
    "        lambda_dict = {}\n",
    "        \n",
    "        for subj in subj_list:\n",
    "            s = f\"{int(subj):02d}\"\n",
    "\n",
    "            input_dict[f\"x{s}\"] = all_subj_voxels[f\"subj{s}\"][data[1][f\"subj{s}\"]].to(device)\n",
    "            lambda_dict[f\"lambda_{s}\"] = all_subj_lambdas[int(s)-1]\n",
    "\n",
    "        loss = model.train(input_dict | lambda_dict)\n",
    "        train_loss += loss\n",
    "\n",
    "        if epoch == epochs:\n",
    "            recon_dict_batch = get_recon_dict_batch(input_dict)\n",
    "            train_metrics_dict = update_metrics(train_metrics_dict, input_dict, recon_dict_batch, n_batches)\n",
    "\n",
    "    train_loss = train_loss / n_batches\n",
    "    print('Epoch: {} Train loss: {:.4f}'.format(epoch, train_loss))\n",
    "    return train_loss, train_metrics_dict\n",
    "\n",
    "def test(epoch):\n",
    "    test_loss = 0\n",
    "    n_batches = len(test_dl)\n",
    "    test_metrics_dict = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_dl):\n",
    "            input_dict = {}\n",
    "            lambda_dict = {}\n",
    "            \n",
    "            for subj in subj_list:\n",
    "                s = f\"{int(subj):02d}\"\n",
    "                input_dict[f\"x{s}\"] = all_subj_voxels[f\"subj{s}\"][data[1][f\"subj{s}\"]].to(device)\n",
    "                lambda_dict[f\"lambda_{s}\"] = all_subj_lambdas[int(s)-1]\n",
    "\n",
    "            loss = model.test(input_dict | lambda_dict)\n",
    "            test_loss += loss\n",
    "\n",
    "            if epoch == epochs:\n",
    "                recon_dict_batch = get_recon_dict_batch(input_dict)\n",
    "                test_metrics_dict = update_metrics(test_metrics_dict, input_dict, recon_dict_batch, n_batches)\n",
    "\n",
    "    test_loss = test_loss / n_batches\n",
    "    print('Epoch: {} Test loss: {:.4f}'.format(epoch, test_loss))\n",
    "    return test_loss, test_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "856bbc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 419556.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Test loss: 419191.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 409474.9062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Test loss: 407633.9062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train loss: 401342.7188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 16.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Test loss: 401230.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train loss: 395673.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Test loss: 396717.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train loss: 391783.7188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Test loss: 393597.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train loss: 387763.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Test loss: 391010.4062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train loss: 384698.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Test loss: 388814.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train loss: 381465.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Test loss: 386842.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train loss: 378511.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Test loss: 385386.3438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train loss: 375759.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Test loss: 383991.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Train loss: 372976.7812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Test loss: 383115.9062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Train loss: 370451.4062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Test loss: 382224.7188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Train loss: 368262.3438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Test loss: 381596.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Train loss: 366185.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Test loss: 381224.7188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Train loss: 364111.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Test loss: 380674.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Train loss: 362168.2812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Test loss: 380833.5938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Train loss: 360406.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Test loss: 380644.3438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Train loss: 358579.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Test loss: 381936.4062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Train loss: 357381.1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 15.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Test loss: 381923.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Train loss: 356072.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Test loss: 382432.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_metrics_dict = train(epoch)\n",
    "    test_loss, test_metrics_dict = test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4298db38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joint_recon_x01': {'cosine_mean': tensor(0.6697),\n",
       "  'pearson_mean': tensor(0.6453)},\n",
       " 'self_recon_x01': {'cosine_mean': tensor(0.6645),\n",
       "  'pearson_mean': tensor(0.6397)},\n",
       " 'cross_recon_x01__x02': {'cosine_mean': tensor(0.6599),\n",
       "  'pearson_mean': tensor(0.6350)},\n",
       " 'cross_recon_x01__x05': {'cosine_mean': tensor(0.6644),\n",
       "  'pearson_mean': tensor(0.6396)},\n",
       " 'cross_recon_x01__x07': {'cosine_mean': tensor(0.6651),\n",
       "  'pearson_mean': tensor(0.6403)},\n",
       " 'joint_recon_x02': {'cosine_mean': tensor(0.6841),\n",
       "  'pearson_mean': tensor(0.6673)},\n",
       " 'cross_recon_x02__x01': {'cosine_mean': tensor(0.6793),\n",
       "  'pearson_mean': tensor(0.6623)},\n",
       " 'self_recon_x02': {'cosine_mean': tensor(0.6756),\n",
       "  'pearson_mean': tensor(0.6581)},\n",
       " 'cross_recon_x02__x05': {'cosine_mean': tensor(0.6793),\n",
       "  'pearson_mean': tensor(0.6623)},\n",
       " 'cross_recon_x02__x07': {'cosine_mean': tensor(0.6799),\n",
       "  'pearson_mean': tensor(0.6629)},\n",
       " 'joint_recon_x05': {'cosine_mean': tensor(0.7236),\n",
       "  'pearson_mean': tensor(0.7034)},\n",
       " 'cross_recon_x05__x01': {'cosine_mean': tensor(0.7186),\n",
       "  'pearson_mean': tensor(0.6980)},\n",
       " 'cross_recon_x05__x02': {'cosine_mean': tensor(0.7152),\n",
       "  'pearson_mean': tensor(0.6944)},\n",
       " 'self_recon_x05': {'cosine_mean': tensor(0.7186),\n",
       "  'pearson_mean': tensor(0.6980)},\n",
       " 'cross_recon_x05__x07': {'cosine_mean': tensor(0.7192),\n",
       "  'pearson_mean': tensor(0.6987)},\n",
       " 'joint_recon_x07': {'cosine_mean': tensor(0.6881),\n",
       "  'pearson_mean': tensor(0.6614)},\n",
       " 'cross_recon_x07__x01': {'cosine_mean': tensor(0.6824),\n",
       "  'pearson_mean': tensor(0.6553)},\n",
       " 'cross_recon_x07__x02': {'cosine_mean': tensor(0.6778),\n",
       "  'pearson_mean': tensor(0.6504)},\n",
       " 'cross_recon_x07__x05': {'cosine_mean': tensor(0.6824),\n",
       "  'pearson_mean': tensor(0.6553)},\n",
       " 'self_recon_x07': {'cosine_mean': tensor(0.6834),\n",
       "  'pearson_mean': tensor(0.6563)}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cd510298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joint_recon_x01': {'cosine_mean': tensor(0.5428),\n",
       "  'pearson_mean': tensor(0.5063)},\n",
       " 'self_recon_x01': {'cosine_mean': tensor(0.5389),\n",
       "  'pearson_mean': tensor(0.5020)},\n",
       " 'cross_recon_x01__x02': {'cosine_mean': tensor(0.5340),\n",
       "  'pearson_mean': tensor(0.4976)},\n",
       " 'cross_recon_x01__x05': {'cosine_mean': tensor(0.5379),\n",
       "  'pearson_mean': tensor(0.5009)},\n",
       " 'cross_recon_x01__x07': {'cosine_mean': tensor(0.5383),\n",
       "  'pearson_mean': tensor(0.5016)},\n",
       " 'joint_recon_x02': {'cosine_mean': tensor(0.5585),\n",
       "  'pearson_mean': tensor(0.5250)},\n",
       " 'cross_recon_x02__x01': {'cosine_mean': tensor(0.5543),\n",
       "  'pearson_mean': tensor(0.5205)},\n",
       " 'self_recon_x02': {'cosine_mean': tensor(0.5517),\n",
       "  'pearson_mean': tensor(0.5179)},\n",
       " 'cross_recon_x02__x05': {'cosine_mean': tensor(0.5538),\n",
       "  'pearson_mean': tensor(0.5201)},\n",
       " 'cross_recon_x02__x07': {'cosine_mean': tensor(0.5543),\n",
       "  'pearson_mean': tensor(0.5207)},\n",
       " 'joint_recon_x05': {'cosine_mean': tensor(0.6109),\n",
       "  'pearson_mean': tensor(0.5771)},\n",
       " 'cross_recon_x05__x01': {'cosine_mean': tensor(0.6059),\n",
       "  'pearson_mean': tensor(0.5717)},\n",
       " 'cross_recon_x05__x02': {'cosine_mean': tensor(0.6007),\n",
       "  'pearson_mean': tensor(0.5677)},\n",
       " 'self_recon_x05': {'cosine_mean': tensor(0.6056),\n",
       "  'pearson_mean': tensor(0.5726)},\n",
       " 'cross_recon_x05__x07': {'cosine_mean': tensor(0.6065),\n",
       "  'pearson_mean': tensor(0.5725)},\n",
       " 'joint_recon_x07': {'cosine_mean': tensor(0.5929),\n",
       "  'pearson_mean': tensor(0.5564)},\n",
       " 'cross_recon_x07__x01': {'cosine_mean': tensor(0.5870),\n",
       "  'pearson_mean': tensor(0.5499)},\n",
       " 'cross_recon_x07__x02': {'cosine_mean': tensor(0.5826),\n",
       "  'pearson_mean': tensor(0.5453)},\n",
       " 'cross_recon_x07__x05': {'cosine_mean': tensor(0.5863),\n",
       "  'pearson_mean': tensor(0.5497)},\n",
       " 'self_recon_x07': {'cosine_mean': tensor(0.5897),\n",
       "  'pearson_mean': tensor(0.5529)}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209d1ca",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4500ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを合体させて、同じ刺激に対する複数被験者の脳活動をまとめて扱えるようにする（被験者1と被験者2なら問題ない）\n",
    "# 同じ数の刺激がある被験者同士ならそのままの順番で合体させたらいいけど、被験者ごとにtestデータの数も異なるので、共通のところを抽出する必要がある\n",
    "# 使うのはbehavの、0であるcoco_idx（画像の指定に使う）と、5であるglobal_trial（fmriのbetaの特定に使う）\n",
    "\n",
    "# Technical notesが参考になるかも https://cvnlab.slite.page/p/h_T_2Djeid/Technical-notes\n",
    "# shared1000ですら欠損があるので、この時点で弱教師あり学習の手法が必要になる可能性\n",
    "\n",
    "# sub1とsub2だけでいいからDMVAEの実装をやってみてその妥当性を最低限検証する方向性がいいかも\n",
    "\n",
    "# その後、複数被験者だったり、欠損対応するためにどうしたらいいかを考えていく\n",
    "# （DMVAEだけでも欠損対応はおそらく可能だが、欠損の方が多いデータにうまく対応するようにはデザインされてないはずなので、やはり、弱教師あり学習の手法が必要）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e697671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# behavior = {\n",
    "# - \"cocoidx\": int(behav.iloc[jj]['73KID'])-1, #0\n",
    "# - \"subject\": subject,                        #1\n",
    "# - \"session\": int(behav.iloc[jj]['SESSION']), #2\n",
    "# - \"run\": int(behav.iloc[jj]['RUN']),         #3\n",
    "# - \"trial\": int(behav.iloc[jj]['TRIAL']),     #4\n",
    "# - \"global_trial\": int(i * (tar + 1)),        #5\n",
    "# - \"time\": int(behav.iloc[jj]['TIME']),       #6\n",
    "# - \"isold\": int(behav.iloc[jj]['ISOLD']),     #7\n",
    "# - \"iscorrect\": iscorrect,                    #8\n",
    "# - \"rt\": rt, # 0 = no RT                      #9\n",
    "# - \"changemind\": changemind,                  #10\n",
    "# - \"isoldcurrent\": isoldcurrent,              #11\n",
    "# - \"iscorrectcurrent\": iscorrectcurrent,      #12\n",
    "# - \"total1\": total1,                          #13\n",
    "# - \"total2\": total2,                          #14\n",
    "# - \"button\": button,                          #15\n",
    "# - \"shared1000\": is_shared1000,               #16\n",
    "# }\n",
    "\n",
    "# 0 = COCO IDX (73K) (used to index coco_images_224_float16.hdf5)\n",
    "# 1 = SUBJECT\n",
    "# 2 = SESSION\n",
    "# 3 = RUN\n",
    "# 4 = TRIAL\n",
    "# 5 = GLOBAL TRIAL (used to index betas_all_subj_fp32_renorm.hdf5)\n",
    "# 6 = TIME\n",
    "# 7 = ISOLD\n",
    "# 8 = ISCORRECT\n",
    "# 9 = RT\n",
    "# 10 = CHANGEMIND\n",
    "# 11 = ISOLDCURRENT\n",
    "# 12 = ISCORRECTCURRENT\n",
    "# 13 = TOTAL1\n",
    "# 14 = TOTAL2\n",
    "# 15 = BUTTON\n",
    "# 16 = IS_SHARED1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e056d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_split_by_node(urls): return urls\n",
    "\n",
    "# train_url = f\"{data_path}/wds/subj01/new_test/\" + \"0.tar\"\n",
    "\n",
    "# train_data = wds.WebDataset(train_url, resampled=False, nodesplitter=my_split_by_node) \\\n",
    "#                                 .decode(\"torch\") \\\n",
    "#                                 .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\") \\\n",
    "#                                 .to_tuple(\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\")\n",
    "\n",
    "# samples = list(train_data)\n",
    "\n",
    "# # !tar -tf /home/acg17270jl/projects/MindEyeV2/dataset/wds/subj01/new_test/0.tar | head -n 10\n",
    "\n",
    "# import tarfile\n",
    "# import io\n",
    "\n",
    "# tar_path = \"/home/acg17270jl/projects/MindEyeV2/dataset/wds/subj01/new_test/0.tar\"\n",
    "\n",
    "# target_index = \"000000000\" # 0から始まる9桁の数字\n",
    "# target_index_int = int(target_index)\n",
    "# target = f\"sample{target_index}.behav.npy\"  # ファイル名は tar -tf で確認したものに置き換え\n",
    "\n",
    "# with tarfile.open(tar_path, \"r\") as tar:\n",
    "#     fileobj = tar.extractfile(target)\n",
    "#     data = io.BytesIO(fileobj.read())\n",
    "#     arr = np.load(data, allow_pickle=False)\n",
    "#     print(f\"tarの{target_index_int}番目のサンプルのcocoidx: {arr[0][0]}\")\n",
    "#     print(f\"train_dataの{target_index_int}番目のサンプルのcocoidx: {samples[target_index_int][0][0][0]}\")\n",
    "#     if arr[0][0] == samples[target_index_int][0][0][0]:\n",
    "#         print(\"tarとtrain_dataの順番は一致しています\")\n",
    "#     else:\n",
    "#         print(\"tarとtrain_dataの順番は一致していません\")\n",
    "\n",
    "# ローカルに保存されている.tarファイルの中身の順番（stimulus index）と、WebDatasetで読み込んだデータの順番（stimulus index）は同じ\n",
    "# tarの0番目のサンプルのcocoidx: 46002.0\n",
    "# train_dataの0番目のサンプルのcocoidx: 46002.0\n",
    "# tarとtrain_dataの順番は一致しています"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
